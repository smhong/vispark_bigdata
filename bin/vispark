#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# NOTE: Any changes in this file must be reflected in SparkSubmitDriverBootstrapper.scala!

export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
export SPARK_CONF_DIR="${SPARK_CONF_DIR:-"$SPARK_HOME/conf"}"

ORIG_ARGS=("$@")

# Set COLUMNS for progress bar
export COLUMNS=`tput cols`

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64


while (($#)); do
  if [ "$1" = "--deploy-mode" ]; then
    SPARK_SUBMIT_DEPLOY_MODE=$2
  elif [ "$1" = "--properties-file" ]; then
    SPARK_SUBMIT_PROPERTIES_FILE=$2
  elif [ "$1" = "--driver-memory" ]; then
    export SPARK_SUBMIT_DRIVER_MEMORY=$2
  elif [ "$1" = "--driver-library-path" ]; then
    export SPARK_SUBMIT_LIBRARY_PATH=$2
  elif [ "$1" = "--driver-class-path" ]; then
    export SPARK_SUBMIT_CLASSPATH=$2
  elif [ "$1" = "--driver-java-options" ]; then
    export SPARK_SUBMIT_OPTS=$2
  elif [ "$1" = "--master" ]; then
    export MASTER=$2
  fi
  shift
done

DEFAULT_PROPERTIES_FILE="$SPARK_CONF_DIR/spark-defaults.conf"
if [ "$MASTER" == "yarn-cluster" ]; then
  SPARK_SUBMIT_DEPLOY_MODE=cluster
fi
export SPARK_SUBMIT_DEPLOY_MODE=${SPARK_SUBMIT_DEPLOY_MODE:-"client"}
export SPARK_SUBMIT_PROPERTIES_FILE=${SPARK_SUBMIT_PROPERTIES_FILE:-"$DEFAULT_PROPERTIES_FILE"}

# For client mode, the driver will be launched in the same JVM that launches
# SparkSubmit, so we may need to read the properties file for any extra class
# paths, library paths, java options and memory early on. Otherwise, it will
# be too late by the time the driver JVM has started.

if [[ "$SPARK_SUBMIT_DEPLOY_MODE" == "client" && -f "$SPARK_SUBMIT_PROPERTIES_FILE" ]]; then
  # Parse the properties file only if the special configs exist
  contains_special_configs=$(
    grep -e "spark.driver.extra*\|spark.driver.memory" "$SPARK_SUBMIT_PROPERTIES_FILE" | \
    grep -v "^[[:space:]]*#"
  )
  if [ -n "$contains_special_configs" ]; then
    export SPARK_SUBMIT_BOOTSTRAP_DRIVER=1
  fi
fi


RESTART=0

if [ "$RESTART" = 1 ]; then
    #SPARK Worker Restart
    $SPARK_HOME/sbin/stop-slaves.sh
    cp /home/whchoi/config/vispark-env.sh $SPARK_HOME/conf/spark-env.sh
    sleep 3
    $SPARK_HOME/sbin/start-slaves.sh
    sleep 15
fi

#portnum=$((RANDOM%1000+3000))
#echo $portnum

#mpirun -np 8 --host dumbo001,dumbo002,dumbo003,dumbo004,dumbo005,dumbo006,dumbo007,dumbo008 python /home/smhong/Project/vispark/python/pyspark/vislib/gpu_manager.py & 

#$SPARK_HOME/bin/kill_manager.py

#$echo whchoi | sudo -S $SPARK_HOME/bin/halo_manager.py $portnum &
#$SPARK_HOME/bin/halo_manager.py $portnum &
#$SPARK_HOME/bin/halo_combined_manager.py $portnum &

python "$SPARK_HOME"/bin/parser.py "${ORIG_ARGS[0]}" "parsed.py"
#exec "$SPARK_HOME"/bin/parser.py "${ORIG_ARGS[0]}" "parsed.py"
ORIG_ARGS[0]="parsed.py"

#echo ${ORIG_ARGS[@]}
exec "$SPARK_HOME"/bin/spark-class org.apache.spark.deploy.SparkSubmit "${ORIG_ARGS[@]}"
#rm file_name

#mpirun -np 8 --host dumbo001,dumbo002,dumbo003,dumbo004,dumbo005,dumbo006,dumbo007,dumbo008 python /home/smhong/Project/vispark/python/pyspark/vislib/kill_manager.py  
#mpirun -np 8 --host dumbo001,dumbo002,dumbo003,dumbo004,dumbo005,dumbo006,dumbo007,dumbo008 python /home/smhong/Project/vispark/python/pyspark/vislib/kill_manager.py  
#mpirun -np 8 --host dumbo001,dumbo002,dumbo003,dumbo004,dumbo005,dumbo006,dumbo007,dumbo008 python /home/smhong/Project/vispark/python/pyspark/vislib/kill_manager.py  
